# Packet Classifier - Final Project Report
Zonglin Peng

## Environment
- TinyCore Linux: the router between the Android machine and the prime OS and the flow sniffer is set here.
- Android VM: the user terminal for operations.


## Introduction
- analyzeFlows - the script for manually analyze the flows from the Andriod machine.
- classifyFlows - the final classifier that uses the trained SVM to identify the flows.
- dataCollect - the data collector to add labels to the flows as the SVM training set.
- trainSVM - the sklearn trainer for the SVM that uses the training set.
- dataset - a sample data set that was used for SVM training.


## Overview
In this project, we have configured a gateway to analyze the packets sent between the actual internet and a Android phone simulator. In this analysis, we need to first successfully capture packets sent and received between the actual internet and android simulator. Then we need to train a machine learning model to classify packets are generated by which apps. Last we need to put this model on the actual gateway to classify real-time flows in the internet.
Phase Details
### Phase 1:
In phase 1, There’s two main things we need to accomplish. The andriod simulator and the Gateway. The android is easy to configure because the steps are clearly stated on the slides, nothing is tricky there. The relative hard part is the configuration of VM, first thing we need to do is to make it SSH-able, so we have to install Open SSH and create a configuration file for the SSH and its corresponding password. When appending changes we have to use filetool.sh -b to save the changes otherwise it won’t save which we cost some time when we were debugging.
We need to change the static IP and netmask. First create a static interface config call eth1.sh to assign the address. Then the .sh file permissions should be changed to allow read-modify-write. Since we need to make it persist, there are lots of places need to be changed, otherwise a line of command will do.
Next, we installed and configure the DHCP server for DNS cache, iptables and ipv6-kernel package. The configuration is written in udhcpd.conf. After the setup, the DHCP server process should be verified. Then we need to configure our VM to act as DHCP server which when android is connected to VM, it will be assigned a IP address. Remember to start the DHCP server and enable IP forwarding at last After all, we saved everything which everytime TC reboot and these changes remain.
Lesson learned:
During the process of leaning, first we get more knowledge on tinycore and vi cmd mode. we know that we need to save using the cmd line after every time we update the files. Also we know how to change the static IP address and netmask how to configure VM to act like DHCP. Also, when we first time did it, since the tutorial on the internet is often changing the ip address for eth0, so when we operating these steps we get confused and error.
#### Limitation:
For this phase, the only limitation we can think of is that some times there will be random error that we can’t fix. For example, after a while the internet is suddenly disconnected for no reason, and we can’t find the reason, but when we download a new one and do all the same steps again, it recovers. Another one can be the android cannot connect to the internet sometimes after rebooting; switch the internet connection may solve the problem.

### Phase 2:
In phase 2, we need to perform packet sniffing on the gateway, therefore we need to implement software to do this and in this case we chose pyshark and python. There is a tricky part where we installed python 3.6 otherwise they won’t integrate. Meanwhile we also need to enable github expat2 which will allow us to pull build in code for pyshark on github. After the pyshark was setup porperly, we made them persist on every reboot.
After all the configuration is done, we need to write code to perform packet sniffing. In this phase we used the LiveCapture feature where the pyshark can directly sniff and filter the packages from the gateway. For the filter from ether 0 we only desire the IP and TCP/UDP packages. Thanks to the pyshark apply_on_packets method, the packets are handled every time a whole package has been received. Therefore, we implemented a method call print_conversatin_header in logFlows.py that takes a packet at a time from the capture, reads the information including <timestamo><source address><dest address><source port><dest port>and<protocol> obtained from the pyshark library:
• pkt.sniff_timstamp,
• pkt.ip.src, pkt.ip.dest,
• pkt[pkt.transport_layer].srcport,
• pkt[pkt.transport_layer].srcport,
• pkt.transport_layer.
The time stamp indicates where a burst ends; when the time stamp of two packets has a one-sec gap, we will conclude on the burst and print the packet information as require. For<#packets rcvd><#bytes sent><#bytes sent>, we iterate through the package, count the packet numbers and sum up the packet length, and categorized them according to the sending address; if the address is the same as that of the Android (192.168.12.100), then this will be a sent packet. The information of the length is obtain by: pkt.length.
Lesson learned:
The verison of python has to be 3.6 otherwise it won’t work. Also we have to enable github expat2 otherwise those built in cannot be used. Besides the implementation problem, we learned how to perform packet sniffing on the gateway by few lines of codes and get more familiar with the pyshark.LiveCapture.
#### Limitations:
When we did this project, we have some misunderstanding about the delivery, we thought that<#packets rcvd><#bytes sent><#bytes sent> only needed to be print after every burst instead of every flow. But the problem was comprehended and in the following phases the information is put at the expected place.

### Phase 3:
In phase 3, we need to train a machine learning model to classify the packet is from which apps. We used the SVM (supportive vector machine) as the classification algorithm. The features we chose are (in each flow):
• packet number,
• packet length sum,
• packet length variation,
• packet length mean,
• protocol, where 1 indicates TCP and 0 indicates UPD.
The first we did was to modify the logFlows.py from the last phase to capture packets from the five applications and append the information corresponding to the features to a .npy file. We define the first column as the label, where 0 is Youtube, 1 is Wiki, 2 is Fruit Ninjia, 3 is Weather Channel, and 4 is US News. Then the next five columns are the corresponding five features. We collected around 2000 tuples of data. After randomizing the tuples, we sliced 1000 data as the training set to train our model, and the other data as the testing set to ensure the training-testing independency.
By evaluating the sklearn.score on the testing set, we obtained an accuracy score of 0.5 as shown below, where PREDICT indicate the test score.
After a good SVC has been trained, we load the SVC in our python program, read the flow file, obtained from TCPdump, by pyshark,FileCapture and by for-looping each packet, parse the packet information into the features of our SVM, and use sklean.score/predict to find the best possible match of the source. If none of the five apps are match, then the machine will report the packet as unknown source.
Lesson learned:
Features are really important in machine learning because it directly related to accuracy. And during this process we learned a lot about knowledge in machine learning including different model and different things machine learning can do, since we are not that familiar with machine learning at the very beginning.
Additionally, the choice on features is crucial to the prediction accuracy. Admittedly the features we choose in this phase may not be the best, yet it indicates it capability to identify packet sources as shown in the feature below.
#### Limitations:
The SVM is not so accurate; from the testing set (data are independent with the training set) it report the score to be around 0.50. However, when parsing the labels of my pcap file, the SVM tends to see some flows as "Youtube" (as shown below), which can be wrong sometimes. For futures improvements on SVM, first we will increase the training data set; second, find some better ways to vectorize the data; third, constrains the data amount to similiar quantities; forth, avoid underfitting or overfitting caused by the given number of features.


### Phase 4:
At this phase we should the pyshark.LiveCapture feature from phase 2 to replace the FileCapture, while using the packet translation methods we implemented from phase 3, where the information of a packet is parsed and vectorized into the features as define the in SVC. Then at real time the SVM will predict the source of the packet same as phase 3. Similar to phase 3, the information of each packet along with its label are printed after each burst.
Lesson learned:
Again the features are important to a machine learning models, and it will heavily affect the accuracy score. Instead of using those, we use the new (11) features:
• length of first ten packet in each flow
• protocol.
It turns out the accuracy score increases by a little, and a Wiki, Fruit Ninjia, and Weather Channel have a much high prediction accuracy.
#### Limitations:
Training set is limited and biased due to inevitable human factors, therefore, the Weather Channel is well-trained than the others, and the machine tends to believe some flows to belong to the Weather Channel.